{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting large maximum entropy models with simulation - Berger machine translation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is an example with simulation on a tiny problem\n",
    "\n",
    "It demonstrates how to use simulation conceptually and the API of `maxentropy`.\n",
    "\n",
    "As in `example_berger.py`, this is the machine translation example\n",
    "-- English to French -- from the paper 'A maximum entropy approach\n",
    "to natural language processing' by Berger et\n",
    "al., 1996.\n",
    "\n",
    "Consider the translation of the English word 'in' into French.  We\n",
    "notice in a corpus of parallel texts the following facts:\n",
    "\n",
    "    (1)    p(dans) + p(en) + p(à) + p(au cours de) + p(pendant) = 1\n",
    "    (2)    p(dans) + p(en) = 3/10\n",
    "    (3)    p(dans) + p(à)  = 1/2\n",
    "\n",
    "This code finds the probability distribution with maximal entropy\n",
    "subject to these constraints **without enumerating the sample space**,\n",
    "using importance sampling instead.\n",
    "\n",
    "This is way overkill for this tiny problem (which can be solved analytically),\n",
    "but it demonstrates how to use simulation in principle to solve larger problems\n",
    "on a continuous or larger discrete sample space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import maxentropy\n",
    "from maxentropy.maxentutils import dictsampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplespace = ['dans', 'en', 'à', 'au cours de', 'pendant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def f0(x):\n",
    "    return x in samplespace\n",
    "\n",
    "@np.vectorize\n",
    "def f1(x):\n",
    "    return x == 'dans' or x == 'en'\n",
    "\n",
    "@np.vectorize\n",
    "def f2(x):\n",
    "    return x == 'dans' or x == 'à'\n",
    "\n",
    "f = [f0, f1, f2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0('dans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a uniform instrumental distribution for sampling\n",
    "samplefreq = {e: 1 for e in samplespace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_sampler = dictsampler(samplefreq, size=10**5, return_probs='logprob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['à', 'au cours de', 'pendant', ..., 'dans', 'à', 'pendant'],\n",
       "       dtype=object),\n",
       " array([-1.60943791, -1.60943791, -1.60943791, ..., -1.60943791,\n",
       "        -1.60943791, -1.60943791]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(auxiliary_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = maxentropy.BigModel(f, auxiliary_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default: model.algorithm = 'CG'\n",
    "# Can choose from ['CG', 'BFGS', 'LBFGSB', 'Powell', 'Nelder-Mead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set the desired feature expectations\n",
    "K = [1.0, 0.3, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad eval #0\n",
      "  norm of gradient = 0.14579623177572185\n",
      "Function eval # 0\n",
      "  dual is  1.6094379124340996\n",
      "Function eval # 1\n",
      "  dual is  1.5903213447586424\n",
      "Grad eval #1\n",
      "  norm of gradient = 0.11644689345025408\n",
      "Function eval # 2\n",
      "  dual is  1.5564066000530865\n",
      "Grad eval #2\n",
      "  norm of gradient = 0.00647132555535496\n",
      "Iteration # 0\n",
      "Function eval # 3\n",
      "  dual is  1.5564066000530865\n",
      "Function eval # 4\n",
      "  dual is  1.556370150276239\n",
      "Grad eval #3\n",
      "  norm of gradient = 0.004797540291035025\n",
      "Function eval # 5\n",
      "  dual is  1.5563257182949286\n",
      "Grad eval #4\n",
      "  norm of gradient = 0.0007113168828275429\n",
      "Iteration # 1\n",
      "Function eval # 6\n",
      "  dual is  1.5563257182949286\n",
      "Function eval # 7\n",
      "  dual is  1.556325262666814\n",
      "Grad eval #5\n",
      "  norm of gradient = 0.0005697037450739942\n",
      "Function eval # 8\n",
      "  dual is  1.5563244471244049\n",
      "Grad eval #6\n",
      "  norm of gradient = 3.8747927192612445e-06\n",
      "Iteration # 2\n",
      "Function eval # 9\n",
      "  dual is  1.5563244471244049\n",
      "Function eval # 10\n",
      "  dual is  1.5563244471108282\n",
      "Grad eval #7\n",
      "  norm of gradient = 3.1324931531043165e-06\n",
      "Function eval # 11\n",
      "  dual is  1.556324447085303\n",
      "Grad eval #8\n",
      "  norm of gradient = 2.784440455682263e-07\n",
      "Iteration # 3\n",
      "Function eval # 12\n",
      "  dual is  1.556324447085303\n",
      "Function eval # 13\n",
      "  dual is  1.556324447085233\n",
      "Grad eval #9\n",
      "  norm of gradient = 2.1429357340603801e-07\n",
      "Function eval # 14\n",
      "  dual is  1.5563244470851354\n",
      "Grad eval #10\n",
      "  norm of gradient = 4.82029953028218e-08\n",
      "Iteration # 4\n",
      "Function eval # 15\n",
      "  dual is  1.5563244470851354\n",
      "Function eval # 16\n",
      "  dual is  1.5563244470851325\n",
      "Grad eval #11\n",
      "  norm of gradient = 3.7519756054146524e-08\n",
      "Function eval # 17\n",
      "  dual is  1.5563244470851303\n",
      "Grad eval #12\n",
      "  norm of gradient = 2.1290144010419682e-12\n",
      "Iteration # 5\n",
      "Function eval # 18\n",
      "  dual is  1.5563244470851303\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.556324\n",
      "         Iterations: 6\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n"
     ]
    }
   ],
   "source": [
    "model.verbose = True\n",
    "\n",
    "# Fit the model\n",
    "# model.avegtol = 1e-5\n",
    "model.fit(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model parameters are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.79112485e-12, -5.40133852e-01,  4.96790840e-01])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the true distribution\n",
    "print(\"Fitted model parameters are:\")\n",
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallmodel = maxentropy.Model(f, samplespace)\n",
    "smallmodel.setparams(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitted distribution is:\n",
      "\tx = dans            \tp(x) = 0.1847\n",
      "\tx = en              \tp(x) = 0.1124\n",
      "\tx = à               \tp(x) = 0.3170\n",
      "\tx = au cours de     \tp(x) = 0.1929\n",
      "\tx = pendant         \tp(x) = 0.1929\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitted distribution is:\")\n",
    "smallmodel.showdist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = smallmodel.probdist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Desired constraints:\n",
      "\tp['dans'] + p['en'] = 0.3\n",
      "\tp['dans'] + p['à']  = 0.5\n",
      "\n",
      "Actual expectations under the fitted model:\n",
      "\tp['dans'] + p['en'] = 0.29713439115856966\n",
      "\tp['dans'] + p['à']  = 0.5017701049704802\n",
      "\n",
      "Estimated error in constraint satisfaction (should be close to 0):\n",
      "[1.31894495e-13 4.47697435e-13 2.07722728e-12]\n",
      "\n",
      "True error in constraint satisfaction (should be close to 0):\n",
      "[2.22044605e-16 2.86560884e-03 1.77010497e-03]\n"
     ]
    }
   ],
   "source": [
    "# Now show how well the constraints are satisfied:\n",
    "print()\n",
    "print(\"Desired constraints:\")\n",
    "print(\"\\tp['dans'] + p['en'] = 0.3\")\n",
    "print(\"\\tp['dans'] + p['à']  = 0.5\")\n",
    "print()\n",
    "print(\"Actual expectations under the fitted model:\")\n",
    "print(\"\\tp['dans'] + p['en'] =\", p[0] + p[1])\n",
    "print(\"\\tp['dans'] + p['à']  = \" + str(p[0]+p[2]))\n",
    "\n",
    "print(\"\\nEstimated error in constraint satisfaction (should be close to 0):\\n\"\n",
    "        + str(abs(model.expectations() - K)))\n",
    "print(\"\\nTrue error in constraint satisfaction (should be close to 0):\\n\" +\n",
    "        str(abs(smallmodel.expectations() - K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
